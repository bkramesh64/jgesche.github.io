---
layout: splash
title: "Predictive Maintenance Part 2.1 - Gaussian Process Regression"
excerpt:
mathjax: true
author_profile: false
categories: PHM Gaussian Process Particle Filter
related: true
header:
  teaser: /assets/images/Cover/Gaussian.png
published: true
---


Gaussian Process with Particle Filter - Part 1
==============================================

This post provides an introduction to the Gaussian Process and Particle Filtering
for time series prognosis. First we will discuss the working principles of the Gaussian Process. The GP can be introduced from a weight space or function space perspective. I will only consider the weight space approach. The weight space approach is more intuitive.
Then, I will elaborate on the working principles of the Particle Filter. The last section presents the Interacting Multiple Model (IMM) approach. The IMM combines multiple particle filters with the aim of increasing prediction accuracy and robustness. I will split this posts into two parts. The first part will
deal with Gaussian Process Regression and the second part with Particle Filtering and the IMM algorithm.


Gaussian Process
----------------

The Gaussian Process is a supervised learning method used for regression and classifications tasks. For future posts in this PHM series only the regression capabilities of the GP will be relevant.
The Gaussian Process defines a prior probability distribution over functions, without parametrizing the function $$f(u,w)$$ directly. Like a Gaussian distribution the GP is fully defined by its mean $$\mu$$ and covariance $$C\left( x,x'\right) $$. The mean $$\mu$$ is often assumed to be zero.

In the following section we will consider a training set $$\mathcal{D}$$ consisting of $$\mathit{n}$$ observations $$\mathcal{D} =\left\lbrace \left( x_i,y_i\right) \mid i = 1,...,n\right\rbrace $$. Here, $$\textbf{x}$$ is a $$\mathit{D}$$-dimensional input vector and $y$ is a noisy observation of the function $$$$. For simplicity we will assume that we model a linear model:
\begin{eqnarray}
y_i= f(\textbf{x}) + \epsilon = \textbf{x}^T \textbf{w} + \epsilon
\tag{1}
\end{eqnarray}

The additional noise $$\epsilon$$ of the function values shall be gaussian distributed with zero mean and variance $$\sigma^2$$. The vector $$\textbf{w}$$ is a parameter vector of the linear model

\begin{eqnarray}
\epsilon \sim \mathcal{N}\left( 0,\sigma_n^2\right)
\tag{2}
\end{eqnarray}


First we define a zero mean prior over the parameters of the model
\begin{eqnarray}
w \sim \mathcal{N}\left( 0,\Sigma\right)
\tag{3}
\end{eqnarray}

Given the observed values $$\textbf{y}$$ and the parameters $$\textbf{w}$$ we can calculate the likelihood of the observed values.

$$
\begin{eqnarray}
p\left( y \mid U ,\textbf{w}\right) = \prod_{i=1}^n p\left( y_i \mid u_i \textbf{w}\right) = \\
\prod_{i=1}^n \dfrac{1}{\sqrt{2 \pi}\sigma_n}\exp\left( -\dfrac{\left( y_i-u_i^Tw\right) ^2}{2 \sigma_n^2}\right) =\mathcal{N}\left( X^T \textbf{w},\sigma_n^2 I\right)
\tag{4}
\end{eqnarray}
$$

Bayes' theorem gives us the posterior distribution of $$\textbf{w}:$$


$$
\begin{eqnarray}
\text{posterior}= \dfrac{\text{likelihood x prior}}{\text{marginal likelihood}}
\tag{5}
\end{eqnarray}$$		



$$\begin{eqnarray}
p\left( \textbf{w} \mid y,X\right)  = \dfrac{p\left( y \mid X,\textbf{w}\right) p(\textbf{w})}{p\left( y \mid X\right) }
\tag{6}
\end{eqnarray}$$



The marginal likelihood, which normalizes the posterior distribution is given by
$$
\begin{eqnarray}
p\left( y \mid X\right) = \int p\left( y \mid X,\textbf{w}\right) p\left( \textbf{w}\right) d\textbf{w}
\tag{7}
\end{eqnarray}
$$
Plugging the terms for the likelihood, the prior and the marginal likelihood into equation 5 yields

$$\begin{eqnarray}
p\left( \textbf{w} \mid y,X\right)= \dfrac{p(y\mid X \textbf{w})p(\textbf{w})}{p(y \mid X)}=
\dfrac{\mathcal{N}_y\left( X^T \textbf{w},\sigma^2 I_n\right) \mathcal{N}_w\left( 0,\Sigma_p\right) }{\int \mathcal{N}_y\left( X^T \textbf{w},\sigma^2 I_n\right) \mathcal{N}_w\left( 0,\Sigma_p\right) d\textbf{w}}
\tag{8}
\end{eqnarray}$$


$$\begin{eqnarray}
\sim \mathcal{N}_y\left( X^T \textbf{w},\sigma^2 I_n\right) \mathcal{N}_w\left( 0,\Sigma_p\right)
\tag{9}
\end{eqnarray}$$


$$\begin{eqnarray}
= \exp \left( -\dfrac{1}{2}\left( w -\bar{w}\right) ^T\left( \dfrac{1}{\sigma^2}XX^T+\Sigma_p^{-1}\right) \left( w-\bar{w}\right) \right)
\tag{10}
\end{eqnarray}$$


where $$\bar{w} = \sigma^-2 \left(\dfrac{1}{\sigma^2}XX^T+\Sigma_p\right) ^{-1}Xy$$.

Using the abbreviation $$A=\dfrac{1}{\sigma^2}XX^T+\Sigma_p^{-1}$$ we can write the posterior as a Gaussian with mean $$\bar{w}$$ and covariance $$A^{-1}$$

$$\begin{eqnarray}
p\left( \textbf{w} \mid y,X\right)  &= \mathcal{N}_w\left(\bar{w},A^{-1} \right)
\tag{11}
\end{eqnarray}$$

Now our goal is to make predictions using the posterior 11 for new inputs $$x_{\ast}$$ from a test dataset given the training data.
For the predictive distribution $$f(x_\ast)$$  at $$x_\ast$$ we get an expression by averaging over all possible outputs of the linear model:

$$\begin{align}
p\left( f_\ast\mid x_\ast,y,X\right)  =\int p\left( f_*\mid x_*,\textbf{w}\right) p\left( \textbf{w}\mid y,X\right) d\textbf{w} = \mathcal{N}_{f_*}\left( x_*^T\bar{w},x_*^T A^{-1}
x_*\right)
\tag{12}
\end{align}$$



<div style="text-align:center">
<img src="/assets/images/PHM/GP_prior_posterior.pdf" alt="test image size" height="80%" width="80%">
<figcaption>Top left: A GP not conditioned on any data points. The gray shaded area depicts the covariance. Remaining plots: The posterior after conditioning on increasing amount of data points. All plots have the same axes.</figcaption>
</div>



The preceding equations of this section are only valid for the considered linear case. To overcome this limitation we can project the D-dimensional input into higher N-dimensional feature space by applying a basis function $$\phi(x)$$. This allows us to apply the posterior for the linear model in the high dimensional space instead of applying it directly on the input. Substituting $$\phi(x)$$ for $$X$$ in equation 12 yields:

$$\begin{align}
p\left( f_\ast\mid x_\ast,y,\phi(x)\right)   \sim \mathcal{N}_{f_*}\left( \phi(x)_*^T\bar{w},\phi(x)_*^T A^{-1}
\phi(x)_{*}\right)
\tag{13}
\label{eqn:posterior_ext}
\end{align}$$

with $$A=\dfrac{1}{\sigma_n^2}\phi(x)\phi(x)^T+\Sigma_p^{-1}$$. The NxN matrix needs to be inverted before we can use 13 for predictions. The computational complexity of an NxN matrix inversion is $$\mathcal{O}(n^3)$$. We can rewrite equation 13 which conveniently reduce computational overhead (Working directly with the basis function might be computationally faster for small datasets [2]).


$$\begin{align}
p\left( f_\ast\mid x_\ast,y,\phi(x)\right)   \sim \mathcal{N}\left( \phi_\ast\Sigma_p\Phi\left( K+\sigma_n^2 \textbf{I}\right) ^{-1}\textbf{y},\phi_\ast\ast\Sigma_p\phi_\ast-\phi_\ast^T\Sigma_{p}\Phi\left( K+\sigma_n^2 \textbf{I}\right) ^{-1}\Phi^T\Sigma_{p}\phi_\ast\right)
\tag{14}
\label{eqn:postior _feature_space}
\end{align}$$

In equation 14 we can find entries of form $$\phi_\ast\Sigma_p\Phi$$,$$\phi_\ast^T\Sigma_p\Phi$$ and $$\phi_\ast^T\Sigma_p\Phi_\ast$$. Thus, we can define $$k(x,x')=\phi(x)^T\Sigma_p\Phi(x')$$. $$k(\cot,\cot)$$ is called a covariance matrix or kernel. The kernel or covariance function has a considerable influence on the generalization properties of the GP. A short overview of different kernels will be provided later in this chapter. Usually the kernel has to be chosen although methods for automatic kernel selection have recently been developed [1].

Using the kernel notation we can write the equations for the mean and covariance  of the GPs posterior:

$$\begin{align}
\mathcal{GP}_{\mu}\left( f_\ast\mid X,y,X_\ast \right)  = k(x,x_\ast)\left[ k(x,x)+\sigma_n^2I\right] ^{-1}y
\tag{15}
\label{eqn:GP_pred_out}
\end{align}$$
$$\begin{align}
\mathcal{GP}_{\Sigma}\left( f_\ast\mid X,y,X_\ast \right)  = k\left(x_\ast,x_\ast\right)  - k\left( x_\ast,x\right) \left[ k(x,x)
+\sigma_n^2 I\right]  ^{-1}k(x,x_\ast)
\tag{16}
\label{eqn:GP_pred_cov}
\end{align}$$

where $$K = K(X,X)$$,$$K_\ast=K(X,X_\ast)$$ and for a single test point $$x_\ast$$ $$k(x_\ast)=k_\ast$$.


Kernels
-------

Kernels for Gaussian process models define the prior covariance between two points $$x$$ and $$x'$$.
Kernel functions $$k(\cdot,\cdot)$$ must meet certain requirements to ensure that they are valid kernels. A kernel k is valid if  the matrix K is positive semi-definite for any set of input values $$x_1,...x_n \in X$$. This guarantees that a mapping $$k(x,x')=\phi(x)^T\Sigma_p\Phi(x')$$. $$k(\cdot,\cdot)$$  does exist.
The choice of the right kernel has a large impact on the performance of the model, since it specifies which functions are likely under the GP prior [1].
Kernels are commonly constructed by combining basic kernels like linear, periodic or squared-exponential kernels. Combining basic kernels ensures that the resulting kernel is positive semi-definte. Constructing kernels from scratch is a more involved process, since it requires proving that matrix K is positive semi-definite. Figure 2 gives an intuition on how the employed kernel affects the respective model. The left plot in Figure 2  was generated using a Gaussian Process with a exponential kernel. For the right plot a squared-exponential kernel was used.


<div style="text-align:center">
<img src="/assets/images/PHM/kernel.pdf" alt="test image size" height="80%" width="80%">
<figcaption>Fig. 2: Left Plot: GP with a exponential kernel. Right Plot:  GP with a squared exponential kernel.</figcaption>
</div>

The squared-exponential kernel is a common kernel which performs well on a lot of problems. This kernel will be used in the next couple of blog posts. The SE-kernel is defined as follows:

$$
\begin{align}
k(x,x')=\sigma_f^2 \exp\left( -\dfrac{(x-x')^2}{2l^2}\right)
\tag{17}
\end{align}$$

The parameters of a kernel are referred to as hyper-parameters. They are not parameters that specifically define a modelled function but a distribution over the function parameters. The hyper-parameters of the SE kernel are the length scale $$l$$, which defines how far the model can extrapolate from the data points. The variance $$\sigma_f^2$$ is merely a scaling factor and determines the average distance from the functions mean.

Implementation in Python
------------------------

Let us implement a Gaussian Process in Python. We start off with loading the necessary libraries.

```python
import numpy as np
from matplotlib import pyplot as plt
```

First, we will define the kernel function. We will implement the squared-exponential Kernel as previously defined.

```python
def Kernel(a,b):
    sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a,b.T)
    return np.exp(-.5*sqdist)
```

Now we can use our kernel function to plot a prior distribution.

```python
N = 10
n = 50

X_test = np.linspace(-5,5,n).reshape(-1,1)
K_ = Kernel(X_test,X_test)

L = np.linalg.cholesky(K_+1e-6*np.eye(n))
f_prior = np.dot(L,np.random.normal(size=(n,10)))

fig = plt.figure()
plt.plot(X_test,f_prior)
plt.show()
```
<div style="text-align:center">
<img src="/assets/images/PHM/prior.png" alt="test image size" height="50%" width="50%">
<figcaption>The prior over functions p(f) .</figcaption>
</div>


Lets break apart what is happening here. We define n=50 test points X_test and calculate the covariance matrix $$K_{*,*}$$.  
Then we calculate the Cholesky decomposition L (which is also known as the "matrix square root"). We use the Cholesky decomposition to express the multivariant normal distribution $$f_* \backsim \mathcal{N}(\mu,\sigma^2)$$ in terms of standard normals: $$f_* \backsim \mu + L\mathcal{N}(0,I)$$. For numerical reasons we have to add small multiples of the identity matrix I to the covariance matrix. The eigenvalues of K can decay rapidly which would cause the Cholesky decomposition to fail.

Now we can try to calculate the mean and covariance at new points $$x_{\ast}$$ using equation 11.

$$\begin{align}
\bar{f}_{\ast}=\textbf{k}_{\ast}^T K^{-1}\textbf{y}
\end{align}$$

The expression within the brackets is usually denoted as $$. We get $$ by solving the linear equation $$\alpha = \textbf{K}_y^{-1}y$$. Using the cholesky decomposition of $$K_y = LL^T$$ allows us to express $$\alpha$$ as
$$\alpha = L^{-T}L^{-1}y$$. Forming the cholesky ensures that the procedure is numerical stable. First, let us generate a function X which we want to approximate. In this case a sin-function.

```python
f = lambda x: np.sin(0.9*x).flatten()

s= 0.00005
X = np.random.uniform(-5, 5, size=(N,1))
print(X.shape)
y = f(X) + s*np.random.randn(N)
K = Kernel(X,X)
L = np.linalg.cholesky(K+s*np.eye(N))
```

In the follwoing code we are not going to calculate $\alpha$ as a intermediate variable. First we solve $$Lk = k_\ast^T L^{-T}$$. In the next line we apply the dot product to
$$Lk^T$$ and the solution of $$L^{-1}y$$.

```python
# Computing the mean'

Lk = np.linalg.solve(L, Kernel(X, X_test))
mu = np.dot(Lk.T, np.linalg.solve(L, y))

# Computing the variance

K_ = Kernel(X_test,X_test)
s2 = np.diag(K_) - np.sum(Lk**2, axis=0)
s = np.sqrt(s2)
print(K_.shape)
```

Now we can plot the posterior of the GP.

```python
plt.figure(1)
plt.clf()
plt.plot(X, y, 'r+', ms=20)
plt.plot(X_test, f(X_test), 'b-')
plt.gca().fill_between(X_test.flat, mu-3*s, mu+3*s, color="#dddddd")
plt.plot(X_test, mu, 'r--', lw=2)
plt.title('Mean predictions plus 3 st.deviations')
plt.axis([-5, 5, -3, 3])
```
<div style="text-align:center">
<img src="/assets/images/PHM/post.png" alt="test image size" height="50%" width="50%">
<figcaption>The posterior after conditioning on our test points</figcaption>
</div>

This concludes the introduction to Gaussian Process Regression. In future posts we will
use the GP in combination with the Particle Filter and Echo State Networks.



References
----------
[1] AVID, Kristjanson D.: Automatic Model Construction with Gaussian Processes, Diss., 2014

[2] BISHOP, Christopher M.: Pattern Recognition and Machine Learning (Information Science and Statistics). Secaucus, NJ, USA : Springer-Verlag New York, Inc., 2006. – ISBN 0387310738

[3] CHEN, Zhe: Bayesian Filtering and From Kalman and Filters to and Particle Filters and Beyond, 2003

[4] ORCHARD, Marcos E.: A Particle Filtering-based Framework for On-line Fault Diagnosis and Failure Prognosis, Diss., 2006

[5] SIMON, D.: Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches. Wiley, 2006 https://books.google.de/books?id=UiMVoP_7TZkC. – ISBN 9780470045336
