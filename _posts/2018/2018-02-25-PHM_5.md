---
layout: splash
title: "Predictive Maintenance Part 5 - Rolling Element Bearing Degradation Dataset"
excerpt:
mathjax: true
author_profile: false
categories: PHM Bearing Degradation EDA
related: true
header:
  teaser: /assets/images/Cover/Maintenance.jpg
published: true
excerpt: Application of Gaussian Process Regression and a Particle Filter for time series prediction.
---


Synthetic Degradation Data
==============

In this post, we will use the previously discussed Gaussian Process and Particle Filter and implement a working prognostic algorithm.
An artificial degradation dataset will be used to analyze the performance of the algorithm. The dataset was generated by Anger [1] for his work on hybrid filter for bearing fault prediction. He used a physics-based degradation model for rolling element bearings that described the growth of a pitting area on a bearing surface. The pitting growth model was then translated into a model for the mechanical vibrations, which was used for the degradation feature extraction.

So let's have a look at the artificial degradation data.


```python
# Load the data
import pandas as pd

deg = pd.read_csv("SimData20deg.csv", sep=',',header=None)
```


```python
print(deg.head(3))
```

       0    1    2    3    4    5    6    7    8    9   ...   377  378  379  380  \
    0  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4 ...   0.0  0.0  0.0  0.0   
    1  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4 ...   0.0  0.0  0.0  0.0   
    2  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4  1.4 ...   0.0  0.0  0.0  0.0   

       381  382  383  384  385  386  
    0  0.0  0.0  0.0  0.0  0.0  0.0  
    1  0.0  0.0  0.0  0.0  0.0  0.0  
    2  0.0  0.0  0.0  0.0  0.0  0.0  

    [3 rows x 387 columns]




```python
deg = deg.round(3)
```

Now let us create some line plots to get a better understanding of the data


```python
%matplotlib inline
import matplotlib.pyplot as plt

fig = plt.figure(figsize = (15,8))
for i in range(1, 7):

    plt.subplot(2,3,i)
    plt.plot(deg.iloc[i,:])

plt.show()
```



<div style="text-align:center">
<img src="/assets/images/PHM/SyntheticData_GPPF/output70.png" alt="test image size" height="60%" width="60%">
</div>



The failure threshold of our system seems to be at 100 and the time span from start to the end of lifetime varies between 97 and 387 timesteps. All degradation trajectories increase monotonically, which will simplify the task of predicting the remaining useful lifetime.

So lets apply the our first model for time series prediction.




# Gaussian Process Regression with Particle Filtering

First, will apply GP regression to build a state update model for the PF.
We will just employ the python code from the previous post on Gaussian Processes.


```python
import numpy as np
```


```python
def Kernel(a,b):
    sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a,b.T)
    return np.exp(-.5*sqdist)
```


```python
def GP(X,X_test,y):
    s= 0.00005
    N= X.shape[0]
    K = Kernel(X,X)
    L = np.linalg.cholesky(K+s*np.eye(N))

    # Computing the mean'

    Lk = np.linalg.solve(L, Kernel(X, X_test))
    mu = np.dot(Lk.T, np.linalg.solve(L, y))

    # Computing the variance

    K_ = Kernel(X_test,X_test)
    s2 = np.diag(K_) - np.sum(Lk**2, axis=0)
    s = np.sqrt(s2)
    return mu,s
```

We want to  drop all zero values which are appended to given degradation time series. We will just truncate the time series after the index  of the maximum value.


```python
series1 = deg.iloc[1,:].truncate(after=deg.iloc[1,:].idxmax())
series2 = deg.iloc[2,:].truncate(after=deg.iloc[2,:].idxmax())
series3 = deg.iloc[3,:].truncate(after=deg.iloc[3,:].idxmax())
```


```python
def numpy_ewma_vectorized_v2(data, window):

    alpha = 2 /(window + 1.0)
    alpha_rev = 1-alpha
    n = data.shape[0]

    pows = alpha_rev**(np.arange(n+1))

    scale_arr = 1/pows[:-1]
    offset = data[0]*pows[1:]
    pw0 = alpha*alpha_rev**(n-1)

    mult = data*pw0*scale_arr
    cumsums = mult.cumsum()
    out = offset + cumsums*scale_arr[::-1]
    return out
```

Before calculating the derivative it might be useful to smooth the functions.
We will apply an exponential moving average filter, although it is not really necessary
for this time series.


```python
y = numpy_ewma_vectorized_v2(np.array(series1), 45)
```


```python
x = np.linspace(1,len(y),(len(y)))
plt.plot(x,y)
plt.show()
```

<div style="text-align:center">
<img src="/assets/images/PHM/SyntheticData_GPPF/output_19_0.png" alt="test image size" height="60%" width="60%">
</div>


```python
y = y[(y>=2)]
x = np.linspace(1,len(y),(len(y)))

# Calculate the gradient of the input function
dy = np.gradient(y)
```


```python
print(y.reshape(-1,1).shape)
print(dy.reshape(-1,1).shape)
```

    (215, 1)
    (215, 1)



```python
plt.plot(y,dy)
plt.show()
```


<div style="text-align:center">
<img src="/assets/images/PHM/SyntheticData_GPPF/output_22_0.png" alt="test image size" height="60%" width="60%">
</div>


## Run the GP


```python
y = y.reshape(-1,1)
dy = dy.reshape(-1,1)
n = 1
X_test = np.linspace(20,100,n).reshape(-1,1)
print(X_test)
mu,s = GP(y,X_test,dy)
print(mu)
```

    [[ 20.]]
    [[ 0.31424421]]


## Run PF


```python
from numpy.random import randn,random
def gaussian_particles(mean, std, N):
    particles = np.empty((N, 1))
    particles[:, 0] = mean + (random(N) * std)
    return particles
```


```python
def predict(particles,y,dy, dt=1.):
    N = len(particles)
    mu, s = GP(y, particles, dy)

    particles = particles + ((mu * dt) + (random(N) * s).reshape(-1, 1))
    return particles


def update(particles, weights, z, R):
    weights.fill(1.)
    weights = stats.norm(particles, R).pdf(z)
    weights += 1.e-300
    weights /= sum(weights)
    return weights.reshape(-1)


def estimate(particles, weights):
    """returns mean and variance of the weighted particles"""
    pos = particles[:,0]
    mean = np.average(pos, weights=weights, axis=0)
    var  = np.average((pos - mean)**2, weights=weights, axis=0)
    return mean, var


def neff(weights):
    return 1. / np.sum(np.square(weights))


def simple_resample(particles, weights):
    N = len(particles)
    cumulative_sum = np.cumsum(weights)
    cumulative_sum[-1] = 1. # avoid round-off error
    indexes = np.searchsorted(cumulative_sum, random(N))
    # resample according to indexes
    particles[:] = particles[indexes]
    weights.fill(1.0 / N)
```

Now that we have all functions loaded we can finally start predicting. One major difference
between the previously discussed implementation of the particle filter and the application for time series prediction is the absence of measurements that we can use
for updating and resampling of the particle distribution. Therefore, multi-step-ahead
prediction only requires a looped application of the predict function.


```python
def particleFilter(y,dy,y_0,threshold):
    N = 50  # Number of particles
    # initial state
    x_N = 0.1  # Noise covariance in the system
    x_R = 0.1  # Noise covariance in the measurement
    dt = 1
    T = y.shape[0]  # Time steps
    # State update model

    # Initial particles, gaussian distributed
    particles = gaussian_particles(y_0, x_N, N).reshape(-1,1)

        # weights = np.ones(N)*1/N
    x_mu = []
    x_var = []
    xparticles = []

    for t in range(T):
            # measurement. We just add some random sensor noise
            true_state = y[t]

            # predict particles
            particles = predict(particles,y,dy)

            # updated Observation
            # incorporate measurements and update our belief- posterior
            # weights = update(particles, weights, z=z, R=x_R)


            xparticles.append(particles)

            x_mu.append(np.mean(particles))
            x_var.append(np.std(particles))

            if x_mu[-1]- 3 * x_var[-1] >= threshold:
                break
    return x_mu,x_var


def plotPrediction(x_mu,x_var,y,threshold):                
    T = len(y)

    x_mu = np.array(x_mu)

    # Truncate of all mean values larger than the threshold value.
    x_mucut = x_mu[(x_mu<=threshold)]


    # Plot failure threshold
    plt.axhline(y=threshold, color='r', linestyle='-')

    # Plot the predicted state
    tx1cut = np.linspace(1, len(x_mucut), len(x_mucut))
    p1 = plt.plot(tx1cut, x_mucut, label='predcited state', c='b',linewidth=1.0)

    tx1 = np.linspace(1, len(x_mu), len(x_mu))
    plt.gca().fill_between(tx1.flat, x_mu - 3 * np.array(x_var), x_mu + 3 * np.array(x_var), color="#dddddd")


    # Plot the true state
    tx2 = np.linspace(1, T, T)
    p2 = plt.plot(tx2, y,label='true state', c='k',linewidth=1.0)
    plt.margins(0)
    plt.gca().fill_between(tx1.flat,threshold , 120, color="#ffffff")
    leg = plt.legend()
    plt.show()

y = y.reshape(-1,1)
dy = dy.reshape(-1, 1)
y_0 = y[1]
threshold = y[-1,0]
print(y_0)
x_mu,x_var = particleFilter(y,dy,y_0,threshold)
plotPrediction(x_mu,x_var,y,threshold)
```

    [ 2.93756784]


<div style="text-align:center">
<img src="/assets/images/PHM/SyntheticData_GPPF/output_29_1.png" alt="test image size" height="60%" width="60%">
</div>



This looks pretty good, which is not surprising since we use the same data for training and prediction.
But our goal is the calculation of the remaining useful lifetime - RUL. We can simply do that by counting the timesteps it takes for the particles to cross the failure threshold.


```python
RUL = len(x_mucut)
```


```python
print(RUL)
```

    214


For this prediction run, the RUL is 214. Now we can make predictions at different timesteps. We will just select 5 evenly spaced timesteps to start the RUL predictions.  


```python
#

threshold = y[-1]
RUL = []
RULtrue = []
RUL_max = len(y)- len(y[(y<=2)])
for i in range(5):
    index = int(len(y)/5) * i
    y_0 = y[index] # Select the start value
    RULtrue.append((len(y)-index)/(RUL_max))
    # Run the particleFilter
    x_mu,x_var = particleFilter(y,dy,y_0,threshold)
    RUL.append(len(x_mu)/RUL_max)


print("The actual remaining useful RUL: {}".format(RULtrue))
print("The predicted remaining useful RUL: {}".format(RUL))
```

    The actual remaining useful RUL: [1.0, 0.8, 0.6, 0.4, 0.2]
    The predicted remaining useful RUL: [0.9906976744186047, 0.7953488372093023, 0.5953488372093023, 0.3953488372093023, 0.2]


After we have calculated the RUL, we can load our previously discussed performance metrics and check how well our model performed.


```python
#α − λ Accuracy
def alpha_lambda(RULTrue, predictedRUL):
    alpha = 0.2
    lambda1 = 1
    alpha_lamda_flag = []
    alpha_upper = RULTrue * (1+alpha);

    alpha_lower = RULTrue * (1-alpha);
    for i in range(len(RULTrue)):
        if predictedRUL[i] < alpha_upper[i] and predictedRUL[i] > alpha_lower[i]:
            alpha_lamda_flag.append(1)
        else:
            alpha_lamda_flag.append(0)

    return alpha_lamda_flag
```


```python
RULTrue = np.array(RULtrue)
predictedRUL = np.array(RUL)
alpha_lamda_flag = alpha_lambda(RULTrue, predictedRUL)
print(alpha_lamda_flag)
```

    [1, 1, 1, 1, 1]



```python
def plotAlphaLambda(RULTrue, predictedRUL):
    # Plot alpha-lambda metric
    alpha = 0.2
    alpha_upper = RULTrue * (1+alpha);

    alpha_lower = RULTrue * (1-alpha);
    x = np.linspace(0,1,len(RULTrue))
    # Create a figure of given size
    fig = plt.figure(figsize=(10,8))
    # Add a subplot
    ax = fig.add_subplot(111)
    plt.rcParams.update({'font.size': 20})
    plt.tick_params(axis='both', which='major', labelsize=20)
    plt.tick_params(axis='both', which='minor', labelsize=20)
    # Set title
    plt.plot(x,RULTrue,label="True RUL")


    plt.gca().fill_between(x, alpha_lower,alpha_upper, color="#dddddd")

    plt.scatter(x,predictedRUL,label="Predicted RUL")

    ax.set_xticks([0,0.2,0.4,0.6,0.8,1])
    plt.margins(0)
    ax.set_xlabel("Time index", fontsize=20)
    ax.set_ylabel("RUL", fontsize=20)
    leg = plt.legend()

plotAlphaLambda(RULTrue, predictedRUL)
```

<div style="text-align:center">
<img src="/assets/images/PHM/SyntheticData_GPPF/output_38_0.png" alt="test image size" height="60%" width="60%">
</div>

```python
#Python code for prognostic metrics - Cumulative Relative Accuracy
def cum_RA(RULTrue, predictedRUL):

    w_r = 1 # weight factor function of the RUL

    #Calculation of Relative Accuracy

    RA =  1 - abs((RULTrue-predictedRUL)/RULTrue);

    p_lambda = len(RA);

    # Cumulative Relative Accuracy
    CRA = (1/abs(p_lambda))*sum(w_r*RA);

    return CRA

CRA = cum_RA(RULTrue, predictedRUL)
print(CRA)
```

    0.993100775194



## Summary

We loaded and analyzed a set of artifical degradation data. Then we used GP Regression
and a Particle Filter to predict the RUL. The performance of the predictive model was then
evaluated by applying the  α − λ and Cumulative Relative Accuracy metric.




References
----------

[1] ANGER, Christoph: Evaluation of hybrid Filter for bearing fault prediction, Diplomarbeit, 2012
