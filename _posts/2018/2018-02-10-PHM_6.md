---
layout: splash
title: "Predictive Maintenance Part 6 - Gaussian Process Echo State Network"
excerpt:
mathjax: true
author_profile: false
categories: PHM GaussianProcess ParticleFilter EchoStateNetworks
header:
  teaser: /assets/images/Cover/Maintenance.jpg
published: false
---

Implementation of a Gaussian Process Echo State Network
=======================================================

First we will initialize the ESN cell.

```python
class ESNCell(rnn_cell_impl.RNNCell):
  def __init__(self, num_units, wr2_scale=0.7, connectivity=0.1, leaky=1.0, activation=math_ops.tanh,
             win_init=init_ops.random_normal_initializer(),
             wr_init=init_ops.random_normal_initializer(),
             bias_init=init_ops.random_normal_initializer()):
  """Initialize the Echo State Network Cell.
  Args:
    num_units: Int or 0-D Int Tensor, the number of units in the reservoir
    wr2_scale: desired norm2 of reservoir weight matrix.
      `wr2_scale < 1` is a sufficient condition for echo state property.
    connectivity: connection probability between two reservoir units
    leaky: leaky parameter
    activation: activation function
    win_init: initializer for input weights
    wr_init: used to initialize reservoir weights before applying connectivity mask and scaling
    bias_init: initializer for biases
  """
  self._num_units = num_units
  self._leaky = leaky
  self._activation = activation

  def _wr_initializer(shape, dtype, partition_info=None):
    wr = wr_init(shape, dtype=dtype)

    connectivity_mask = math_ops.cast(
        math_ops.less_equal(
          random_ops.random_uniform(shape),
          connectivity),
      dtype)

    wr = math_ops.multiply(wr, connectivity_mask)

    wr_norm2 = math_ops.sqrt(math_ops.reduce_sum(math_ops.square(wr)))

    is_norm_0 = math_ops.cast(math_ops.equal(wr_norm2, 0), dtype)

    wr = wr * wr2_scale / (wr_norm2 + 1 * is_norm_0)

    return wr

  self._win_initializer = win_init
  self._bias_initializer = bias_init
  self._wr_initializer = _wr_initializer
```


After initializing the ESN cell we define the __call__ method.

```python
@property
def output_size(self):
  return self._num_units

@property
def state_size(self):
  return self._num_units

def __call__(self, inputs, state, scope=None):
  """ Run one step of ESN Cell
      Args:
        inputs: `2-D Tensor` with shape `[batch_size x input_size]`.
        state: `2-D Tensor` with shape `[batch_size x self.state_size]`.
        scope: VariableScope for the created subgraph; defaults to class `ESNCell`.
      Returns:
        A tuple `(output, new_state)`, computed as
        `output = new_state = (1 - leaky) * state + leaky * activation(Win * input + Wr * state + B)`.
      Raises:
        ValueError: if `inputs` or `state` tensor size mismatch the previously provided dimension.
        """

  inputs = convert_to_tensor(inputs)
  input_size = inputs.get_shape().as_list()[1]
  dtype = inputs.dtype

  with vs.variable_scope(scope or type(self).__name__):  # "ESNCell"

    win = vs.get_variable("InMatrix", [input_size, self._num_units], dtype=dtype,
                          trainable=False, initializer=self._win_initializer,reuse=tf.AUTO_REUSE)
    wr = vs.get_variable("ReservoirMatrix", [self._num_units, self._num_units], dtype=dtype,
                         trainable=False, initializer=self._wr_initializer)
    b = vs.get_variable("Bias", [self._num_units], dtype=dtype, trainable=False, initializer=self._bias_initializer)
    #Design matrix
    in_mat = array_ops.concat([inputs, state], axis=1)
    weights_mat = array_ops.concat([win, wr], axis=0)

    output = (1 - self._leaky) * state + self._leaky * self._activation(math_ops.matmul(in_mat, weights_mat) + b)

  return output, output

```

After we have set up the ESN class, we can instantiate the ESN class.
The method tf.nn.dynamic_rnn creates a recurrent neural network specified
by RNNCell cell. The input tensor is of shape: [batch_size, max_time, ...].
In this case the batch size is 1 and max_time defined by elements.

```python
elements=2000
data_t = tf.reshape(tf.constant(data), [1, elements, 1])
esn = ESNCell(units, connectivity, scale)

print("Building graph...")
outputs, final_state = tf.nn.dynamic_rnn(esn, data_t, dtype=tf.float32)
washed = tf.squeeze(tf.slice(outputs, [0, washout_size, 0], [-1, -1, -1]))
summary = tf.summary.merge_all()
```

Next we call tf.Session. A tensorflwo session is an encapsulates the environement in which Oepration objects are executed and tensors evaluated. A session can own
resources like variables or queues, which have to be released if they are not needed. You can invoke .close() on the session or use a context manager. When we exit the with block, all resources allocated on entry are released regardless of what happened during execution of code within the with block.

The command  S.run(tf.global_variables_initializer()) runs the variable's initializer operation, which initializes our variables.
Finally, we calculate the reservoir state by calling S.run(washed).
Afterwards the state matrix is split into two parts. The matrix tr_state contains reservoir state for the first 500 timesteps of training.
Ts_state


``` python
with tf.Session() as S:
  summary_writer = tf.summary.FileWriter(train_dir, S.graph)
  S.run(tf.global_variables_initializer())

  print("Computing embeddings...")
  res = S.run(washed)
 # summary_writer.add_summary(res)
  print("Computing direct solution...")
  state = np.array(res)
  tr_state = np.mat(state[:tr_size])
  ts_state = np.mat(state[tr_size:])
  wout = np.transpose(np.mat(data[washout_size+1:tr_size+washout_size+1]) * np.transpose(np.linalg.pinv(tr_state)))
```

Line 12 calculates the output weights $$W_{out}$$ by inverting the tr_state matrix.
We want to use a Gaussian Process instead of a matrix inversion to get $$W_{out}$$.




## Hyperparameter optimization of the GP

We optimize the Kernel Hyperparameter $$\sigma$$ by minimzing the model evidence

$$\begin{align}
\ln P(y|x, \theta) = -\frac{1}{2}\ln|K| - \frac{1}{2}y^tK^{-1}y - \frac{N}{2}\ln{2\pi}
\end{align}$$

To minimize the log marignal likelihood we need the derivative

$$\begin{align}
\frac{\partial}{\partial\theta_i} \log P(y|x, \theta) = \frac{1}{2}y^TK^{-1}\frac{\partial K}{\partial\theta_i}K^{-1}y^T
-\frac{1}{2}\mathrm{tr}\left(K^{-1}\frac{\partial K}{\partial\theta_i}\right)
\end{align}$$


We use a radial basis function as Kernel. The derivatives with respect to $$\sigma$$ and $$l$$
are:

$$\begin{align}
\frac{\partial K}{\partial\sigma} = 2\sigma\exp\left(\frac{-(x-x')^T(x-x')}{2l^2}\right)
\end{align}$$


$$\begin{align}
\frac{\partial K}{\partial l} = \sigma^2\exp\left(\frac{-(x-x')^T(x-x')}{2l^2}\right) \frac{(x-x')^T(x-x')}{l^3}
\end{align}$$
