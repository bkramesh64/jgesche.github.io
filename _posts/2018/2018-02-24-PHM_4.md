---
layout: splash
title: "Predictive Maintenance Part 4 - Performance Metrics"
excerpt:
mathjax: true
author_profile: false
categories: PHM BearingDegradation EDA Performance
header:
  teaser: /assets/images/Cover/alpha_lambda.png
published: true
excerpt: Introduction to performance metrics in PHM and implementation of the α − λ and CRA metric.
---



Performance Metrics
===================

Performance of a prognostic algorithm can, in general, be assessed by evaluating the predicated and actual remaining useful lifetime. The development of prognostic algorithms calls for standardized methodology and a set of reliable metrics. Unfortunately, standard metrics for PHM applications have not yet been defined. Most research teams apply performance metrics which are specific to their requirements in their respective fields [1].
The absence of such standards makes it difficult to define design requirements, compare the performance of algorithms and leave room for ambiguous interpretation.

<div style="text-align:center">
<img src="/assets/images/PHM/Introduction/metrics_classes.png" alt="test image size" height="50%" width="50%">
<figcaption>Fig. 1: This  figure depicts the classification of performance metrics [1].</figcaption>
</div>


However, the performance metrics classification framework proposed by Saxena u. a. [1] is a good starting point for the comparison of PHM algorithms. Within this framework, performance metrics can be categorized based on the function they fulfill into the three categories: algorithm performance, computational performance, and cost-benefit. The category algorithm performance is of main interest to us. This category is subdivided into the classes: accuracy, robustness, precision, and trajectory (see figure 1). However, the trajectory metric is not clearly defined and was therefore not considered. The metrics we will use are α − λ Accuracy, Cumulative Relative Accuracy (CRA) and the Mean Absolute Deviation (MAD). The following listing gives an in detail description of each metric:

-  **Accuracy based metric: α − λ Accuracy**

α − λ Accuracy is a binary metric that indicates if a predicted value lies within defined α bounds at a specific time step λt. The value λt is the fraction of time between the time of prediction $$t_p$$ and the end of lifetime $$t_{EOL}$$. The metric can be formalized as follows:

  $$\begin{align}
   	\alpha-\lambda-  \text{Accuracy}=
          \begin{cases}
          1 \quad\text{if} \quad[1-\alpha] r_*(t_\lambda) \leq r^l(t_\lambda) \leq \left[ 1 +\alpha \right] r_*(t_\lambda)\\
          0 \quad   \qquad\text{otherwise}
          \end{cases}
    \end{align}$$

where $$t_\lambda$$ is defined as:

$$\begin{align}
    t_\lambda = t_P + \lambda \left( EoL - t_P\right)
  \end{align}$$

For instance, $$t_{0.5}$$ implies that half of the components lifetime is over. This notation allows easy
comparison of algorithms for a given λ. Throughout the next posts, we set α = 0.2.

<div style="text-align:center">
<img src="/assets/images/PHM/Introduction/alpha_lambda.png" alt="test image size" height="30%" width="30%">
<figcaption>Fig. 2: Plot depicting the alpha lambda accuracy metric. The blue shaded area represents the accuracy cone [2].</figcaption>
</div>



   - **Accuracy based metric: Cumulative Relative Accuracy (CRA)**

Relative Accuracy (RA) is a metric similar to the $$\alpha-\lambda$$ Accuracy, but instead of outputting a binary value the RA quantifies the prediction accuracy. RA is formalized as:


$$\begin{align}
   	\text{RA}_\lambda = 1- \dfrac{\left|r_\ast(t_\lambda)-r^l(t_\lambda)\right|  }{r_\ast(t_\lambda)}
   	\end{align}$$


The Cumulative Relative Accuracy then gives an impression of the general performance of the algorithm since the RA only gives information about the algorithm performance at a specific point in time $$t_\lambda$$. The CRA is defined as a normalized weighted sum of relative prediction accuracies at specific instances [1]:


$$\begin{align}
   	\text{CRA}_\lambda = \dfrac{1}{\left| l_\lambda \right| }\sum_{i=1}^{l_\lambda}w(r^l)\cdot RA_\lambda
\end{align}$$


where $$l_\lambda $$ is the set of all time indexes before the prediction is made and $$\mid l_\lambda \mid$$ is the cardinality of the set. $$w(r^l)$$ is a set of weights which allows us to give accuracies close to EOL a higher importance. The CRA is restricted a range between 0 and 1 where CRA= 1 is a perfect score. For a prediction error larger than $$100\%$$ the CRA is set to zero.


- **Precision based metric: Mean absolute deviation from the sample median**

 The Mean absolute deviation from the sample median (MAD) is a resistant estimator of the dispersion or spread of the error. If the sample size is expected to be low or outliers might be encountered, the MAD is a robust metric that allows making meaningful inferences [1].  It is defined as follows:


$$\begin{align}
 	AD(i)=\dfrac{1}{n} \sum_{l=1}^n \mid \Delta^l(i) - M \mid
 	\end{align}$$

  where $$M = \underset{l}{\mathrm{median}}  \left( \Delta^l(i)\right)$$ and median is the $$\dfrac{n+1}{2}$$ th order statistic. The perfect MAD score is 0.

- **Robustness based metric**

Robustness metrics quantify the sensitivity of prognostic algorithms to inherent variations of the input data. For this task, the influence of the number of training datasets on the performance of the prognostic algorithm was evaluated.  



A detailed discussion of performance metrics and their application can be found in [1].

## Implementation

The implementation of both performance metrics in python is pretty straight forward. Therefore I will just post the code. First, let us implement the α − λ Accuracy metric.


###  α − λ Accuracy

```python
#α − λ Accuracy
def alpha_lambda(RULTrue, predictedRUL):
    alpha = 0.2
    lambda1 = 1
    alpha_lamda_flag = []
    alpha_upper = RULTrue * (1+alpha);

    alpha_lower = RULTrue * (1-alpha);
    for i in range(len(RULTrue)):
        if predictedRUL[i] < alpha_upper[i] and predictedRUL[i] > alpha_lower[i]:
            alpha_lamda_flag.append(1)
        else:
            alpha_lamda_flag.append(0)

    return alpha_lamda_flag



```


```python
import numpy as np
import pandas as pd
RULTrue =np.array([1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0])
predictedRUL = np.array([0.9,0.8,0.63,0.6,0.55,0.6,0.45,0.3,0.2,0.08,0.02])
alpha_lamda_flag = alpha_lambda(RULTrue, predictedRUL)
print(alpha_lamda_flag)
```

    [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0]



```python
# Plot alpha-lambda metric
%matplotlib inline
import matplotlib.pyplot as plt
RULTrue =np.array([1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0])
predictedRUL = np.array([0.9,0.8,0.63,0.6,0.55,0.6,0.45,0.3,0.2,0.08,0.02])
alpha = 0.2
alpha_upper = RULTrue * (1+alpha);

alpha_lower = RULTrue * (1-alpha);
x = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# Create a figure of given size
fig = plt.figure(figsize=(10,8))
# Add a subplot
ax = fig.add_subplot(111)
plt.rcParams.update({'font.size': 20})
plt.tick_params(axis='both', which='major', labelsize=20)
plt.tick_params(axis='both', which='minor', labelsize=20)
# Set title
plt.plot(x,RULTrue,label="True RUL")


plt.gca().fill_between(x, alpha_lower,alpha_upper, color="#dddddd")

plt.scatter(x,predictedRUL,label="Predicted RUL")

ax.set_xticks([0,0.2,0.4,0.6,0.8,1])
plt.margins(0)
ax.set_xlabel("Time index", fontsize=20)
ax.set_ylabel("RUL", fontsize=20)
leg = plt.legend()


```

<div style="text-align:center">
<img src="/assets/images/PHM/Introduction/RUL.png" alt="test image size" height="60%" width="60%">
<figcaption>Fig. 3: RUL prediction based on RULTrue and predictedRUL. Predictions which are within the shaded cone will be labeled as "true" and dots outside the shaded cone as "false" by the α − λ Accuracy metric.</figcaption>
</div>



### Cumulative Relative Accuracy

Next we will implement the Cumulative Relative Accuracy or CRA.


```python
#Python code for prognostic metrics - Cumulative Relative Accuracy
def cum_RA(RULTrue, predictedRUL):

    w_r = 1 # weight factor function of the RUL

    #Calculation of Relative Accuracy

    RA =  1 - abs((RULTrue-predictedRUL)/RULTrue);

    p_lambda = len(RA);

    # Cumulative Relative Accuracy
    CRA = (1/abs(p_lambda))*sum(w_r*RA);

    return CRA
```


```python
CRA = cum_RA(RULTrue, predictedRUL)
print(CRA)
```

    0.0204166666667


    Now let's do some quick sanity checks by considering two extreme cases: perfect prediction and zero RUL prediction.


```python
RULTrue =np.array([120,100,80,60,40,20])
predictedRUL = np.array([120,100,80,60,40,20])

alpha_lamda_flag = alpha_lambda(RULTrue, predictedRUL)
print("Perfect prediction result α − λ Accuracy: {}".format(alpha_lamda_flag))

CRA = cum_RA(RULTrue, predictedRUL)
print("Perfect prediction result CRA {}".format(CRA))
```

    Perfect prediction result α − λ Accuracy: [1, 1, 1, 1, 1, 1]
    Perfect prediction result CRA 1.0



```python
RULTrue =np.array([120,100,80,60,40,20])
predictedRUL = np.array([0,0,0,0,0,0])

alpha_lamda_flag = alpha_lambda(RULTrue, predictedRUL)
print("Zero prediction result α − λ Accuracy: {}".format(alpha_lamda_flag))

CRA = cum_RA(RULTrue, predictedRUL)
print("Zero prediction result CRA: {}".format(CRA))
```

    Zero prediction result α − λ Accuracy: [0, 0, 0, 0, 0, 0]
    Zero prediction result CRA: 0.0


The results align with our expectations. Now we are ready to apply these metrics.

References
----------

[1] SAXENA, Abhinav ; CELAYA, Jose ; BALABAN, Edward ; GOEBEL, Kai ; SAHA, Bhaskar ; SAHA, Sankalita ; SCHWABACHER, Mark: Metrics for Evaluating Performance of Prognostic Techniques. (2008)

[2] SHANE, Butler: Prognostic Algorithms and for Condition Monitoring and Remaining Useful Life Estiamtion, Diss., 2012
